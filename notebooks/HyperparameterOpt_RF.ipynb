{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Cti3DiCwIjX1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "import warnings\n",
        "from matplotlib import pyplot as plt\n",
        "import copy\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pass the path of your final CSV file path to `MERGED_CSV_FILE_PATH` that has merged data from 2019, 2020, and 2021. We train one single model for all 3 years combined\n",
        "\n",
        "#### This notebook was run with a high CPU count machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C1zh0UPYIlAr"
      },
      "outputs": [],
      "source": [
        "MERGED_CSV_FILE_PATH = \"YOUR_MERGED_FOLDER_PATH/merged_2019_2020_2021.csv\"\n",
        "combined_dataframe = pd.read_csv(MERGED_CSV_FILE_PATH)\n",
        "combined_dataframe = combined_dataframe.drop(combined_dataframe.columns[0], axis=1)\n",
        "combined_dataframe['agbd_points'] = combined_dataframe['agbd_points'].apply(ast.literal_eval)\n",
        "combined_dataframe['overlap'] = combined_dataframe['overlap'].apply(ast.literal_eval)\n",
        "list_ = combined_dataframe.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ERXI5DzNInic"
      },
      "outputs": [],
      "source": [
        "dummies = pd.get_dummies(combined_dataframe.Ecoregion_l3)\n",
        "combined_dataframe_new = combined_dataframe.join(dummies)\n",
        "list_ = combined_dataframe_new.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FhzLJ3e4IrJ5"
      },
      "outputs": [],
      "source": [
        "def clean_up_list(l, overlap_percentage=0.5):\n",
        "  l = l.copy()\n",
        "  filtered_list = []\n",
        "  for i in l:\n",
        "    agbd = i[18]\n",
        "    overlap = i[23]\n",
        "    filtered_agbd = [value for value, b_value in zip(agbd, overlap) if b_value >= overlap_percentage]\n",
        "    i[18] = filtered_agbd.copy()\n",
        "  return l.copy()\n",
        "\n",
        "filterd_list = clean_up_list(copy.deepcopy(list_.copy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add the value for `BEST_MEAN_COUNT` that was found from FeatureSelectionBulkOverlap.ipynb notebook\n",
        "#### Add the indices for the best features that was found from FeatureSelectionBulkOverlap.ipynb notebook "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BEST_MEAN_COUNT = 14\n",
        "BEST_FEATURES = [3, 9, 12, 14, 19, 21, 24]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjyvon0hIxLw"
      },
      "outputs": [],
      "source": [
        "filtered_by_region_mean = []\n",
        "for index_, row in enumerate(filterd_list):\n",
        "  agbd_values = row[18]\n",
        "  if len(agbd_values) >= BEST_MEAN_COUNT:\n",
        "    if all(value >= 0 for value in agbd_values):\n",
        "      mean_agbd = sum(agbd_values) / len(agbd_values)\n",
        "      values_to_add = [row[i] for i in BEST_FEATURES + list(range(26, 44))] \n",
        "      # 3, 9, 12, 14, 19, 21, 24 are the best features found from FeatureSelectionBulkOverlap.ipynb notebook. Be sure to add the index of columns correctly\n",
        "      # 3 = yeojohnson\n",
        "      # 9 = yeojohnson\n",
        "      # 12 = yeojohnson\n",
        "      # 14 = yeojohnson\n",
        "      # 19 = None\n",
        "      # 21 = yeojohnson\n",
        "      # 24 = yeojohnson\n",
        "      # These transformations were done by manually checking skewness\n",
        "      # The absolute lowest skewness measure was chosen \n",
        "      values_to_add.append(math.log(mean_agbd))\n",
        "      filtered_by_region_mean.append(values_to_add)\n",
        "\n",
        "data_array = np.array(filtered_by_region_mean)\n",
        "X, y = data_array[:, :-1], data_array[:, -1]\n",
        "transformer_dict = {idx: PowerTransformer(method='yeo-johnson') for idx in [0, 1, 2, 3, 5, 6]}  # Map column indices to transformers\n",
        "for idx in transformer_dict:\n",
        "  X[:, idx] = transformer_dict[idx].fit_transform(X[:, idx].reshape(-1, 1)).flatten()\n",
        "Q1 = np.percentile(y, 25)\n",
        "Q3 = np.percentile(y, 75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "y = np.where((y < lower_bound) | (y > upper_bound), np.nan, y)\n",
        "X = X[~np.isnan(y)]\n",
        "y= y[~np.isnan(y)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### We do training in two parts since having just one models leads to overpredicting or underpredicting values\n",
        "#### First fit an RF with training set\n",
        "#### Compute residuals = y_train - y_pred\n",
        "#### Fit an RF with these residuals as dependent variable against training set\n",
        "#### While predection first predict from RF model 1, then predict from RF model 2\n",
        "#### Add the results of both the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZbBLBHfpRAHK"
      },
      "outputs": [],
      "source": [
        "rf_hyperparam_grid={\n",
        "    \"n_estimators\": [100, 200, 250, 500],\n",
        "    \"max_features\": ['auto', 7, 10, 15],\n",
        "    \"min_samples_leaf\": [1, 5, 20, 100],\n",
        "    \"min_samples_split\": [2, 10, 50, 250],\n",
        "    \"criterion\": [\"absolute_error\", \"squared_error\"],\n",
        "    \"max_depth\": [4, 6, 8, 10, None]\n",
        "}\n",
        "cv_split = KFold(n_splits=10, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EZPvtlXvUycs"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aTR-7cQwPmo-"
      },
      "outputs": [],
      "source": [
        "rf_random_search=RandomizedSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42),\n",
        "    param_distributions=rf_hyperparam_grid,\n",
        "    n_iter=1,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    refit=True,\n",
        "    return_train_score=True,\n",
        "    cv=cv_split,\n",
        "    verbose=10,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "tuned_random_model = rf_random_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Copy the best parameters for the Random Forest for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gzZhBqHTdTb",
        "outputId": "95e106cb-9b05-40d3-a1c1-95719517627a"
      },
      "outputs": [],
      "source": [
        "tuned_random_model.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UMJGOuEqVL_d"
      },
      "outputs": [],
      "source": [
        "random_y_hat = tuned_random_model.predict(X_train)\n",
        "residuals = y_train - random_y_hat\n",
        "cv_split = KFold(n_splits=10, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xlEYF6to51Wz"
      },
      "outputs": [],
      "source": [
        "rf_random_search_bias=RandomizedSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42),\n",
        "    param_distributions=rf_hyperparam_grid,\n",
        "    n_iter=1,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    refit=True,\n",
        "    return_train_score=True,\n",
        "    cv=cv_split,\n",
        "    verbose=10,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "tuned_bias_random_model = rf_random_search_bias.fit(X_train, residuals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Copy the best parameters for the bias estimator Random Forest for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "U38KSdhR3ln2"
      },
      "outputs": [],
      "source": [
        "tuned_bias_random_model.best_params_"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
