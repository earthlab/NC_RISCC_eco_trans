{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50482552",
   "metadata": {
    "_cell_guid": "6aef052a-4a38-4558-a57d-1e7fdfda6267",
    "_uuid": "cd1f4cbb-4690-4f2b-aef9-d4130c39ba21",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-17T05:54:50.947632Z",
     "iopub.status.busy": "2024-06-17T05:54:50.947194Z",
     "iopub.status.idle": "2024-06-17T05:54:51.689317Z",
     "shell.execute_reply": "2024-06-17T05:54:51.688241Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.752162,
     "end_time": "2024-06-17T05:54:51.692215",
     "exception": false,
     "start_time": "2024-06-17T05:54:50.940053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from google.oauth2.credentials import Credentials\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff173da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T05:54:51.704430Z",
     "iopub.status.busy": "2024-06-17T05:54:51.702981Z",
     "iopub.status.idle": "2024-06-17T05:54:51.709667Z",
     "shell.execute_reply": "2024-06-17T05:54:51.708474Z"
    },
    "papermill": {
     "duration": 0.015197,
     "end_time": "2024-06-17T05:54:51.712217",
     "exception": false,
     "start_time": "2024-06-17T05:54:51.697020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Details to get Google Earth Engine API access\n",
    "# https://www.kaggle.com/code/pcjimmmy/how-to-get-the-secret-and-tokens-and-access\n",
    "credentials = Credentials(\n",
    "        None,\n",
    "        refresh_token = \"MY_REFRESH_TOKEN\",\n",
    "        token_uri=ee.oauth.TOKEN_URI,\n",
    "        client_id=\"MY_CLIENT_ID\",\n",
    "        client_secret=\"MY_CLIENT_SECRET\",\n",
    "        scopes=ee.oauth.SCOPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48147ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T05:54:51.724264Z",
     "iopub.status.busy": "2024-06-17T05:54:51.723029Z",
     "iopub.status.idle": "2024-06-17T05:54:52.721054Z",
     "shell.execute_reply": "2024-06-17T05:54:52.719603Z"
    },
    "papermill": {
     "duration": 1.007282,
     "end_time": "2024-06-17T05:54:52.724182",
     "exception": false,
     "start_time": "2024-06-17T05:54:51.716900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ee.Initialize(credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c9b28",
   "metadata": {},
   "source": [
    "#### Define the `year_` variable your GEDI L4A data is from\n",
    "#### As mentioned before, we downloaded GEDI data for a pre-defined **region**. Provide the GEE path of that region data in `pre_defined_region_path_on_gee` variable\n",
    "#### We created a merged .shp file of all the downloaded GEDI L4A data and uploaded it to GEE. Provide the GEE path of the merged GEDI L4A data in `merged_gedi_l4a_path_on_gee`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf80797",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T05:54:52.735043Z",
     "iopub.status.busy": "2024-06-17T05:54:52.734592Z",
     "iopub.status.idle": "2024-06-17T05:54:52.743525Z",
     "shell.execute_reply": "2024-06-17T05:54:52.742078Z"
    },
    "papermill": {
     "duration": 0.017412,
     "end_time": "2024-06-17T05:54:52.746126",
     "exception": false,
     "start_time": "2024-06-17T05:54:52.728714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the datasets\n",
    "year_ = 2021\n",
    "\n",
    "pre_defined_region_path_on_gee = \"projects/ee-mygeeusername/assets/NC_forests_all_time\"\n",
    "all_nc_forests = ee.FeatureCollection(pre_defined_region_path_on_gee)\n",
    "\n",
    "merged_gedi_l4a_path_on_gee = \"projects/ee-mygeeusername/assets/merged_precision_2021\"\n",
    "table = ee.FeatureCollection(merged_gedi_l4a_path_on_gee)\n",
    "\n",
    "\n",
    "modis_product = ee.ImageCollection(\"MODIS/061/MCD43A4\")\n",
    "aqua_primary_gpp_npp = ee.ImageCollection(\"MODIS/061/MYD17A3HGF\")\n",
    "LAI_FPAR = ee.ImageCollection(\"MODIS/061/MCD15A3H\")\n",
    "terra_primary_gpp_npp = ee.ImageCollection(\"MODIS/061/MOD17A3HGF\")\n",
    "ecoregionsL3 = ee.FeatureCollection(\"EPA/Ecoregions/2013/L3\")\n",
    "usgs_land_cover = ee.Image(\"USGS/NLCD_RELEASES/2020_REL/NALCMS\");\n",
    "all_nc_forests = all_nc_forests.sort(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a762fad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T05:54:52.757378Z",
     "iopub.status.busy": "2024-06-17T05:54:52.756731Z",
     "iopub.status.idle": "2024-06-17T05:54:52.764127Z",
     "shell.execute_reply": "2024-06-17T05:54:52.762510Z"
    },
    "papermill": {
     "duration": 0.016181,
     "end_time": "2024-06-17T05:54:52.766850",
     "exception": false,
     "start_time": "2024-06-17T05:54:52.750669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "year_to_calculate_start = year_\n",
    "year_to_calculate_end = year_\n",
    "start_date = ee.Date.fromYMD(year_to_calculate_start, 1, 1)\n",
    "end_date = ee.Date.fromYMD(year_to_calculate_end, 12, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b5dd5",
   "metadata": {},
   "source": [
    "#### Since we did not use GCP buckets to save the data, and used Google Drive instead, create a folder with the value of `GDRIVE_FOLDER` variable. For E.g. we created a folder called **MAPPED_GEDI_MODIS_DATA_2021** The mapped GEDI-MODIS data was saved in this folder. Make sure you do all these Google operations on the same account. By default GEE has access to your drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe54479",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDRIVE_FOLDER = \"MAPPED_GEDI_MODIS_DATA_2021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c563708b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T05:54:52.779252Z",
     "iopub.status.busy": "2024-06-17T05:54:52.778798Z",
     "iopub.status.idle": "2024-06-17T05:54:52.848249Z",
     "shell.execute_reply": "2024-06-17T05:54:52.846870Z"
    },
    "papermill": {
     "duration": 0.080153,
     "end_time": "2024-06-17T05:54:52.851612",
     "exception": false,
     "start_time": "2024-06-17T05:54:52.771459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_mp(index_to_run):\n",
    "  try:\n",
    "    global GDRIVE_FOLDER\n",
    "    time.sleep(random.randrange(1,3))\n",
    "    modis_projection = modis_product.first().projection()\n",
    "\n",
    "    merged_forests = all_nc_forests.map(lambda feature: feature.transform(modis_projection, ee.ErrorMargin(0.001)))\n",
    "    agbd_table = table.map(lambda feature: feature.transform(modis_projection, ee.ErrorMargin(0.001)))\n",
    "\n",
    "    merged_forests = ee.FeatureCollection(merged_forests.toList(12219).slice(index_to_run, index_to_run + 1))\n",
    "    agbd_table = ee.FeatureCollection(agbd_table)\n",
    "\n",
    "    roi = merged_forests.geometry()\n",
    "    specific_feature = agbd_table.filterBounds(merged_forests.geometry())\n",
    "    specific_feature = specific_feature.filter(ee.Filter.eq('l4_quality', '1'))\n",
    "    feature_spec = specific_feature.getInfo()\n",
    "    if len(feature_spec[\"features\"]) == 0:\n",
    "      print(\"Skipping for index {} since there are no values\".format(index_to_run))\n",
    "      return\n",
    "\n",
    "    # Define calculation functions\n",
    "    def calculate_ndvi(image):\n",
    "        ndvi = image.normalizedDifference([\"Nadir_Reflectance_Band2\", \"Nadir_Reflectance_Band1\"])\n",
    "        return image.addBands(ndvi.rename(\"NDVI\"))\n",
    "\n",
    "    def calculate_evi(image):\n",
    "        G = 2.5\n",
    "        C1 = 6\n",
    "        C2 = 7.5\n",
    "        L = 1\n",
    "        red = image.select('Nadir_Reflectance_Band1')\n",
    "        nir = image.select('Nadir_Reflectance_Band2')\n",
    "        blue = image.select('Nadir_Reflectance_Band3')\n",
    "        evi = nir.multiply(0.0001).subtract(red.multiply(0.0001)).multiply(G).divide(\n",
    "            nir.multiply(0.0001).add(red.multiply(0.0001).multiply(C1)).subtract(\n",
    "                blue.multiply(0.0001).multiply(C2)).add(L))\n",
    "        return evi.rename('EVI')\n",
    "\n",
    "    def calculate_ndwi(image):\n",
    "        ndwi = image.normalizedDifference([\"Nadir_Reflectance_Band2\", \"Nadir_Reflectance_Band5\"])\n",
    "        return ndwi.rename(\"NDWI\")\n",
    "\n",
    "    def clip_function(image):\n",
    "        return image.clip(roi)\n",
    "\n",
    "    def filter_bands_ndvi(image):\n",
    "        mask_red = image.select(\"BRDF_Albedo_Band_Mandatory_Quality_Band1\").eq(0)\n",
    "        mask_nir = image.select(\"BRDF_Albedo_Band_Mandatory_Quality_Band2\").eq(0)\n",
    "        return image.updateMask(mask_red).updateMask(mask_nir)\n",
    "\n",
    "    def filter_bands_evi(image):\n",
    "        mask_red = image.select(\"BRDF_Albedo_Band_Mandatory_Quality_Band1\").eq(0)\n",
    "        mask_nir = image.select(\"BRDF_Albedo_Band_Mandatory_Quality_Band2\").eq(0)\n",
    "        mask_blue = image.select(\"BRDF_Albedo_Band_Mandatory_Quality_Band3\").eq(0)\n",
    "        return image.updateMask(mask_nir).updateMask(mask_red).updateMask(mask_blue)\n",
    "\n",
    "    def filter_bands_ndwi(image):\n",
    "        mask_nir = image.select(\"BRDF_Albedo_Band_Mandatory_Quality_Band2\").eq(0)\n",
    "        mask_swir = image.select(\"BRDF_Albedo_Band_Mandatory_Quality_Band5\").eq(0)\n",
    "        return image.updateMask(mask_nir).updateMask(mask_swir)\n",
    "\n",
    "    def mask_clouds_in_fpar(image):\n",
    "        qc = image.select('FparLai_QC')\n",
    "        cloud_state = qc.bitwiseAnd(int('11000', 2)).rightShift(3)\n",
    "        mask = cloud_state.eq(0)\n",
    "        return image.select(\"Fpar\").updateMask(mask)\n",
    "\n",
    "    def mask_clouds_in_lai(image):\n",
    "        qc = image.select('FparLai_QC')\n",
    "        cloud_state = qc.bitwiseAnd(int('11000', 2)).rightShift(3)\n",
    "        mask = cloud_state.eq(0)\n",
    "        return image.select(\"Lai\").updateMask(mask)\n",
    "\n",
    "    # Compute mean values for each product\n",
    "    modis_ndvi_mean = modis_product.map(clip_function).filterDate(start_date, end_date).filter(\n",
    "        ee.Filter.calendarRange(6, 9, 'month')).map(filter_bands_ndvi).map(calculate_ndvi).select(\n",
    "        \"NDVI\").reduce(ee.Reducer.mean()).rename(\"NDVI_mean\").reproject(crs=modis_projection)\n",
    "\n",
    "    modis_evi_mean = modis_product.map(clip_function).filterDate(start_date, end_date).filter(\n",
    "        ee.Filter.calendarRange(6, 9, 'month')).map(filter_bands_evi).map(calculate_evi).select(\n",
    "        \"EVI\").reduce(ee.Reducer.mean()).rename(\"EVI_mean\").reproject(crs=modis_projection)\n",
    "\n",
    "    modis_ndwi_mean = modis_product.map(clip_function).filterDate(start_date, end_date).filter(\n",
    "        ee.Filter.calendarRange(6, 9, 'month')).map(filter_bands_ndwi).map(calculate_ndwi).select(\n",
    "        \"NDWI\").reduce(ee.Reducer.mean()).rename(\"NDWI_mean\").reproject(crs=modis_projection)\n",
    "\n",
    "    modis_fpar_mean = LAI_FPAR.select(\"Fpar\", \"FparLai_QC\").map(clip_function).filterDate(\n",
    "        start_date, end_date).filter(ee.Filter.calendarRange(6, 9, 'month')).map(mask_clouds_in_fpar).select(\n",
    "        \"Fpar\").reduce(ee.Reducer.mean()).rename(\"Fpar_mean\").reproject(crs=modis_projection)\n",
    "\n",
    "    modis_lai_mean = LAI_FPAR.select(\"Lai\", \"FparLai_QC\").map(clip_function).filterDate(\n",
    "        start_date, end_date).filter(ee.Filter.calendarRange(6, 9, 'month')).map(mask_clouds_in_lai).select(\n",
    "        \"Lai\").reduce(ee.Reducer.mean()).rename(\"Lai_mean\").reproject(crs=modis_projection)\n",
    "\n",
    "    # Similarly compute annual means\n",
    "    modis_ndvi_mean_annual = modis_product.map(clip_function).filterDate(start_date, end_date).map(\n",
    "        filter_bands_ndvi).map(calculate_ndvi).select(\"NDVI\").reduce(ee.Reducer.mean()).rename(\n",
    "        \"NDVI_mean_annual\").reproject(crs=modis_projection)\n",
    "\n",
    "    modis_evi_mean_annual = modis_product.map(clip_function).filterDate(start_date, end_date).map(\n",
    "        filter_bands_evi).map(calculate_evi).select(\"EVI\").reduce(ee.Reducer.mean()).rename(\n",
    "        \"EVI_mean_annual\").reproject(crs=modis_projection)\n",
    "\n",
    "    modis_ndwi_mean_annual = modis_product.map(clip_function).filterDate(start_date, end_date).map(\n",
    "        filter_bands_ndwi).map(calculate_ndwi).select(\"NDWI\").reduce(ee.Reducer.mean()).rename(\n",
    "        \"NDWI_mean_annual\").reproject(crs=modis_projection)\n",
    "\n",
    "    modis_fpar_mean_annual = LAI_FPAR.select(\"Fpar\", \"FparLai_QC\").map(clip_function).filterDate(\n",
    "        start_date, end_date).map(mask_clouds_in_fpar).select(\"Fpar\").reduce(ee.Reducer.mean()).rename(\n",
    "        \"Fpar_mean_annual\").reproject(crs=modis_projection)\n",
    "\n",
    "    modis_lai_mean_annual = LAI_FPAR.select(\"Lai\", \"FparLai_QC\").map(clip_function).filterDate(\n",
    "        start_date, end_date).map(mask_clouds_in_lai).select(\"Lai\").reduce(ee.Reducer.mean()).rename(\n",
    "        \"Lai_mean_annual\").reproject(crs=modis_projection)\n",
    "\n",
    "    terra_gpp_mean_annual = terra_primary_gpp_npp.select(\"Gpp\").map(clip_function).filterDate(\n",
    "        start_date, end_date).reduce(ee.Reducer.mean()).multiply(0.0001).rename(\n",
    "        \"Terra_gpp_mean_annual\").reproject(crs=modis_projection)\n",
    "\n",
    "    aqua_gpp_mean_annual = aqua_primary_gpp_npp.select(\"Gpp\").map(clip_function).filterDate(\n",
    "        start_date, end_date).reduce(ee.Reducer.mean()).multiply(0.0001).rename(\n",
    "        \"Aqua_gpp_mean_annual\").reproject(crs=modis_projection)\n",
    "\n",
    "    terra_npp_mean_annual = terra_primary_gpp_npp.select(\"Npp\").map(clip_function).filterDate(\n",
    "        start_date, end_date).reduce(ee.Reducer.mean()).multiply(0.0001).rename(\n",
    "        \"Terra_npp_mean_annual\").reproject(crs=modis_projection)\n",
    "\n",
    "    aqua_npp_mean_annual = aqua_primary_gpp_npp.select(\"Npp\").map(clip_function).filterDate(\n",
    "        start_date, end_date).reduce(ee.Reducer.mean()).multiply(0.0001).rename(\n",
    "        \"Aqua_npp_mean_annual\").reproject(crs=modis_projection)\n",
    "\n",
    "    # DEM processing\n",
    "    dem = ee.Image('NASA/NASADEM_HGT/001').select('elevation').updateMask(ee.Image('NASA/NASADEM_HGT/001').select('elevation').gt(0)).clip(roi)\n",
    "    slope = ee.Terrain.slope(dem)\n",
    "    aspect = ee.Terrain.aspect(dem)\n",
    "\n",
    "    dem_resampled = dem.reduceResolution(reducer=ee.Reducer.mean(), maxPixels=1024*10).reproject(crs=modis_projection)\n",
    "    slope_resampled = slope.reduceResolution(reducer=ee.Reducer.mean(), maxPixels=1024*10).reproject(crs=modis_projection)\n",
    "    aspect_resampled = aspect.reduceResolution(reducer=ee.Reducer.mean(), maxPixels=1024*10).reproject(crs=modis_projection)\n",
    "\n",
    "    # Combine images\n",
    "    combined_image = modis_ndvi_mean.addBands([\n",
    "        modis_evi_mean, modis_ndwi_mean, modis_fpar_mean, modis_lai_mean, \n",
    "        dem_resampled, slope_resampled, aspect_resampled,\n",
    "        modis_ndvi_mean_annual, modis_evi_mean_annual, modis_ndwi_mean_annual,\n",
    "        modis_fpar_mean_annual, modis_lai_mean_annual, terra_gpp_mean_annual,\n",
    "        aqua_gpp_mean_annual, terra_npp_mean_annual, aqua_npp_mean_annual\n",
    "    ])\n",
    "\n",
    "    slope_mask = slope.lte(30).eq(1)\n",
    "    slope_mask_reproj = slope_mask.reproject(crs=slope.projection())\n",
    "    combined_image = combined_image.updateMask(slope_mask_reproj)\n",
    "\n",
    "    # Feature processing\n",
    "    def error_mask(feature):\n",
    "        agbd = ee.Number.parse(feature.get('agbd'))\n",
    "        agbd_se = ee.Number(feature.get('agbd_se'))\n",
    "        relative_se = agbd_se.divide(agbd)\n",
    "        valid = relative_se.lte(0.5)\n",
    "        return feature.set('valid', valid)\n",
    "\n",
    "    def buffer_agbd_points(feature):\n",
    "        return feature.buffer(12.5).bounds()\n",
    "\n",
    "    filtered_features = specific_feature.map(error_mask).filter(ee.Filter.eq('valid', 1))\n",
    "    filtered_features = filtered_features.map(buffer_agbd_points)\n",
    "\n",
    "    modis_points = combined_image.sample(region=roi, projection=modis_projection, geometries=True)\n",
    "\n",
    "    def bounding_circle_func(feature):\n",
    "        intermediate_buffer = feature.buffer(231.5)\n",
    "        return intermediate_buffer\n",
    "\n",
    "    def bounding_box_func(feature):\n",
    "        intermediate_box = feature.bounds()\n",
    "        return intermediate_box\n",
    "\n",
    "    bounding_circles = modis_points.map(bounding_circle_func)\n",
    "    bounding_boxes = bounding_circles.map(bounding_box_func)\n",
    "\n",
    "    def calculate_overlap_percentage(modis_feature, agbd_feature):\n",
    "        modis_geometry = modis_feature.geometry()\n",
    "        agbd_geometry = agbd_feature.geometry().transform(modis_geometry.projection(), ee.ErrorMargin(0.001))\n",
    "        intersection = modis_geometry.intersection(agbd_geometry, ee.ErrorMargin(0.001))\n",
    "        intersection_area = intersection.area(ee.ErrorMargin(0.001))\n",
    "        gedi_area = agbd_geometry.area(ee.ErrorMargin(0.001))\n",
    "        overlap_percentage = intersection_area.divide(gedi_area)\n",
    "        return agbd_feature.set('overlap_percentage', overlap_percentage)\n",
    "\n",
    "    def sample_points_from_feature_with_all(feature):\n",
    "        sampled_points = filtered_features.filterBounds(feature.geometry())\n",
    "        valid_sampled_points = sampled_points.map(lambda point: calculate_overlap_percentage(feature, point))\n",
    "        valid_sampled_points_size = valid_sampled_points.size()\n",
    "        result = ee.Algorithms.If(\n",
    "            valid_sampled_points_size.gt(0),\n",
    "            feature.set({\n",
    "                'Ecoregion_l1': ee.Algorithms.If(\n",
    "                    ecoregionsL3.filterBounds(feature.geometry()).size().gt(0),\n",
    "                    ecoregionsL3.filterBounds(feature.geometry()).first().get('l1_key'),\n",
    "                    ee.String('None')\n",
    "                ),\n",
    "                'Ecoregion_l2': ee.Algorithms.If(\n",
    "                    ecoregionsL3.filterBounds(feature.geometry()).size().gt(0),\n",
    "                    ecoregionsL3.filterBounds(feature.geometry()).first().get('l2_key'),\n",
    "                    ee.String('None')\n",
    "                ),\n",
    "                'Ecoregion_l3': ee.Algorithms.If(\n",
    "                    ecoregionsL3.filterBounds(feature.geometry()).size().gt(0),\n",
    "                    ecoregionsL3.filterBounds(feature.geometry()).first().get('l3_key'),\n",
    "                    ee.String('None')\n",
    "                ),\n",
    "                'degrade_flag': valid_sampled_points.aggregate_array('degrade_fl'),\n",
    "                'NDVI_mean': feature.get('NDVI_mean'),\n",
    "                'EVI_mean': feature.get('EVI_mean'),\n",
    "                'NDWI_mean': feature.get('NDWI_mean'),\n",
    "                'Fpar_mean': feature.get('Fpar_mean'),\n",
    "                'Lai_mean': feature.get('Lai_mean'),\n",
    "                'elevation': feature.get('elevation'),\n",
    "                'aspect': feature.get('aspect'),\n",
    "                'slope': feature.get('slope'),\n",
    "                'NDVI_mean_annual': feature.get('NDVI_mean_annual'),\n",
    "                'EVI_mean_annual': feature.get('EVI_mean_annual'),\n",
    "                'NDWI_mean_annual': feature.get('NDWI_mean_annual'),\n",
    "                'Fpar_mean_annual': feature.get('Fpar_mean_annual'),\n",
    "                'Lai_mean_annual': feature.get('Lai_mean_annual'),\n",
    "                'Terra_gpp_mean_annual': feature.get('Terra_gpp_mean_annual'),\n",
    "                'Aqua_gpp_mean_annual': feature.get('Aqua_gpp_mean_annual'),\n",
    "                'Terra_npp_mean_annual': feature.get('Terra_npp_mean_annual'),\n",
    "                'Aqua_npp_mean_annual': feature.get('Aqua_npp_mean_annual'),\n",
    "                'agbd_points': valid_sampled_points.aggregate_array('agbd'),\n",
    "                'land_cover_points': valid_sampled_points.aggregate_array('land_cover'),\n",
    "                'overlap': valid_sampled_points.aggregate_array('overlap_percentage')\n",
    "            }),\n",
    "            None\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    sampled_points_collection_with_all = bounding_boxes.map(sample_points_from_feature_with_all, True)\n",
    "    feature_size = sampled_points_collection_with_all.getInfo()\n",
    "\n",
    "    if len(feature_size[\"features\"]) == 0:\n",
    "      print(\"Skipping for index {} since there are no values\".format(index_to_run))\n",
    "      return\n",
    "    fileName = 'Python_sample_points_{}'.format(str(index_to_run))\n",
    "    description = 'exported_sampledPoints_with_NDVI_{}'.format(str(index_to_run))\n",
    "    export_task = ee.batch.Export.table.toDrive(\n",
    "      collection=sampled_points_collection_with_all,\n",
    "      description=description,\n",
    "      folder=GDRIVE_FOLDER,\n",
    "      fileNamePrefix=fileName,  \n",
    "      fileFormat='CSV' \n",
    "    )\n",
    "    export_task.start()\n",
    "    while export_task.active():\n",
    "      task_status = export_task.status()\n",
    "      time.sleep(10)\n",
    "    print(\"task completed for {}\".format(index_to_run))\n",
    "  except Exception as exp:\n",
    "    print(exp)\n",
    "    print(\"task failed for {} with {}\".format(index_to_run, exp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b05d8b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T05:54:52.863314Z",
     "iopub.status.busy": "2024-06-17T05:54:52.862897Z",
     "iopub.status.idle": "2024-06-17T05:54:52.868805Z",
     "shell.execute_reply": "2024-06-17T05:54:52.867484Z"
    },
    "papermill": {
     "duration": 0.015035,
     "end_time": "2024-06-17T05:54:52.871771",
     "exception": false,
     "start_time": "2024-06-17T05:54:52.856736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "limit_range = int(all_nc_forests.size().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b059cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T05:54:52.883686Z",
     "iopub.status.busy": "2024-06-17T05:54:52.883243Z",
     "iopub.status.idle": "2024-06-17T10:58:21.978484Z",
     "shell.execute_reply": "2024-06-17T10:58:21.976655Z"
    },
    "papermill": {
     "duration": 18209.268406,
     "end_time": "2024-06-17T10:58:22.145123",
     "exception": false,
     "start_time": "2024-06-17T05:54:52.876717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_of_prc = 15\n",
    "matrix = list(range(start, limit_range))\n",
    "if __name__ == '__main__':\n",
    "  try:\n",
    "    with Pool(no_of_prc) as p:\n",
    "        p.map(run_mp, matrix)\n",
    "  except Exception as exp:\n",
    "    print(exp)\n",
    "    print(\"Error in something\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae75fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_csv(folder):\n",
    "  csvs_list = []\n",
    "  sorted_csv = sorted(os.listdir(folder), key= lambda x: int(x.split(\"_\")[-1].split(\".csv\")[0]))\n",
    "  for csv in sorted_csv:\n",
    "    csvs_list.append(os.path.join(folder, csv))\n",
    "  return csvs_list\n",
    "\n",
    "csv_2021 = get_all_csv(GDRIVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(csv_list):\n",
    "  dataframes = []\n",
    "  for file_ in csv_list:\n",
    "    if not file_.endswith('.csv'):\n",
    "        continue\n",
    "    df = pd.read_csv(file_)\n",
    "    dataframes.append(df)\n",
    "  print(\"Finished doing batch\")\n",
    "  return dataframes\n",
    "\n",
    "\n",
    "def merge_csv(all_csv, processes = 4):\n",
    "  batch_size = len(all_csv) // processes\n",
    "  all_batches = []\n",
    "  dataframes = []\n",
    "  count = 0\n",
    "  for i in range(0, len(all_csv), batch_size):\n",
    "    batch_files = all_csv[i:i + batch_size]\n",
    "    all_batches.append(batch_files)\n",
    "  with Pool(processes=processes) as pool:\n",
    "    dataframes = pool.map(read_csv, all_batches)\n",
    "  merged_gdf = merge_dataframe(dataframes)\n",
    "  return merged_gdf\n",
    "\n",
    "\n",
    "def merge_dataframe(dataframes):\n",
    "  merged_gdf = None\n",
    "  for frame in dataframes:\n",
    "    batch_gdf = pd.concat(frame, ignore_index=True)\n",
    "    if merged_gdf is None:\n",
    "      merged_gdf = batch_gdf\n",
    "    else:\n",
    "      merged_gdf = pd.concat([merged_gdf, batch_gdf], ignore_index=True)\n",
    "  return merged_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd9e6a",
   "metadata": {},
   "source": [
    "#### Define the `FOLDER_PATH_TO_SAVE_MERGED_CSV_2021` folder variable. This is where the CSV's from your `year` will be merged and stored as one single CSV file\n",
    "#### You need to repeat all the steps until this notebook for year 2019, 2020 and 2021\n",
    "#### Your final CSV file should have merged data from 2019, 2020, and 2021. We train one single model for all 3 years combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6178abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH_TO_SAVE_MERGED_CSV_2021 = \"MERGED_FOLDER_PATH_FOR_CSV\"\n",
    "os.makedirs(FOLDER_PATH_TO_SAVE_MERGED_CSV_2021, exist_ok=True)\n",
    "if __name__ == \"__main__\":\n",
    "  dataframes = merge_csv(csv_2021, processes=32)\n",
    "  dataframes.to_csv(os.path.join(FOLDER_PATH_TO_SAVE_MERGED_CSV_2021, \"merged_2021_overlap.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18215.767165,
   "end_time": "2024-06-17T10:58:23.380970",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-17T05:54:47.613805",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
